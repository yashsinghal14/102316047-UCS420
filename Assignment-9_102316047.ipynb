{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 9 CC By Yash Singhal(102316047)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports,\n",
    "technology, food, books, etc.).\n",
    "1. Convert text to lowercase and remove punctuaƟon.\n",
    "2. Tokenize the text into words and sentences.\n",
    "3. Remove stopwords (using NLTK's stopwords list).\n",
    "4. Display word frequency distribuƟon (excluding stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text:\n",
      "\n",
      "artificial intelligence is transforming various industries making tasks easier and more efficient \n",
      "from healthcare to finance aipowered systems are becoming indispensable tools for decisionmaking \n",
      "technologies like deep learning and machine learning allow for better predictions automated processes and enhanced data analysis\n",
      "\n",
      "\n",
      "Tokens:\n",
      "['artificial', 'intelligence', 'is', 'transforming', 'various', 'industries', 'making', 'tasks', 'easier', 'and', 'more', 'efficient', 'from', 'healthcare', 'to', 'finance', 'aipowered', 'systems', 'are', 'becoming', 'indispensable', 'tools', 'for', 'decisionmaking', 'technologies', 'like', 'deep', 'learning', 'and', 'machine', 'learning', 'allow', 'for', 'better', 'predictions', 'automated', 'processes', 'and', 'enhanced', 'data', 'analysis']\n",
      "\n",
      "Filtered Words:\n",
      "['artificial', 'intelligence', 'transforming', 'various', 'industries', 'making', 'tasks', 'easier', 'efficient', 'healthcare', 'finance', 'aipowered', 'systems', 'becoming', 'indispensable', 'tools', 'decisionmaking', 'technologies', 'like', 'deep', 'learning', 'machine', 'learning', 'allow', 'better', 'predictions', 'automated', 'processes', 'enhanced', 'data', 'analysis']\n",
      "Word Frequency Distribution (Excluding Stopwords):\n",
      "Counter({'learning': 2, 'artificial': 1, 'intelligence': 1, 'transforming': 1, 'various': 1, 'industries': 1, 'making': 1, 'tasks': 1, 'easier': 1, 'efficient': 1, 'healthcare': 1, 'finance': 1, 'aipowered': 1, 'systems': 1, 'becoming': 1, 'indispensable': 1, 'tools': 1, 'decisionmaking': 1, 'technologies': 1, 'like': 1, 'deep': 1, 'machine': 1, 'allow': 1, 'better': 1, 'predictions': 1, 'automated': 1, 'processes': 1, 'enhanced': 1, 'data': 1, 'analysis': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yashs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yashs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yashs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "# NLTK resources \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "Artificial Intelligence is transforming various industries, making tasks easier and more efficient. \n",
    "From healthcare to finance, AI-powered systems are becoming indispensable tools for decision-making. \n",
    "Technologies like deep learning and machine learning allow for better predictions, automated processes, and enhanced data analysis.\n",
    "\"\"\"\n",
    "\n",
    "# 1. Convert text to lowercase and remove punctuation\n",
    "text_lower = text.lower()\n",
    "text_clean = text_lower.translate(str.maketrans('', '', string.punctuation))\n",
    "print(\"Cleaned Text:\")\n",
    "print(text_clean)\n",
    "\n",
    "# 2. Tokenize the text into words and sentence\n",
    "words = word_tokenize(text_clean)\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"\\nTokens:\")\n",
    "print(words)\n",
    "\n",
    "# 3. Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "print(\"\\nFiltered Words:\")\n",
    "print(filtered_words)\n",
    "\n",
    "# 4. Display word frequency distribution\n",
    "word_freq = Counter(filtered_words)\n",
    "print(\"Word Frequency Distribution (Excluding Stopwords):\")\n",
    "print(word_freq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Stemming and LemmaƟzaƟon\n",
    "1. Take the tokenized words from QuesƟon 1 (aŌer stopword removal).\n",
    "2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
    "3. Apply lemmaƟzaƟon using NLTK's WordNetLemmaƟzer.\n",
    "4. Compare and display results of both techniques. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemming (PorterStemmer): ['artifici', 'intellig', 'transform', 'variou', 'industri', 'make', 'task', 'easier', 'effici', 'healthcar', 'financ', 'aipow', 'system', 'becom', 'indispens', 'tool', 'decisionmak', 'technolog', 'like', 'deep', 'learn', 'machin', 'learn', 'allow', 'better', 'predict', 'autom', 'process', 'enhanc', 'data', 'analysi']\n",
      "\n",
      "Stemming (LancasterStemmer): ['art', 'intellig', 'transform', 'vary', 'industry', 'mak', 'task', 'easy', 'efficy', 'healthc', 'fin', 'aipow', 'system', 'becom', 'indispens', 'tool', 'decisionmak', 'technolog', 'lik', 'deep', 'learn', 'machin', 'learn', 'allow', 'bet', 'predict', 'autom', 'process', 'enh', 'dat', 'analys']\n",
      "\n",
      "Lemmatization: ['artificial', 'intelligence', 'transforming', 'various', 'industry', 'making', 'task', 'easier', 'efficient', 'healthcare', 'finance', 'aipowered', 'system', 'becoming', 'indispensable', 'tool', 'decisionmaking', 'technology', 'like', 'deep', 'learning', 'machine', 'learning', 'allow', 'better', 'prediction', 'automated', 'process', 'enhanced', 'data', 'analysis']\n"
     ]
    }
   ],
   "source": [
    "# Initialize stemmers and lemmatizer\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply stemming using PorterStemmer and LancasterStemmer\n",
    "porter_stemmed = [porter_stemmer.stem(word) for word in filtered_words]\n",
    "lancaster_stemmed = [lancaster_stemmer.stem(word) for word in filtered_words]\n",
    "\n",
    "# Apply lemmatization using WordNetLemmatizer\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "# Compare and display results of both techniques\n",
    "print(\"\\nStemming (PorterStemmer):\", porter_stemmed)\n",
    "print(\"\\nStemming (LancasterStemmer):\", lancaster_stemmed)\n",
    "print(\"\\nLemmatization:\", lemmatized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Regular Expressions and Text Spliƫng\n",
    "1. Take their original text from QuesƟon 1.\n",
    "2. Use regular expressions to:\n",
    "a. Extract all words with more than 5 leƩers.\n",
    "b. Extract all numbers (if any exist in their text).\n",
    "c. Extract all capitalized words.\n",
    "3. Use text spliƫng techniques to:\n",
    "a. Split the text into words containing only alphabets (removing digits and special\n",
    "characters).\n",
    "b. Extract words starƟng with a vowel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words with more than 5 letters: ['artificial', 'intelligence', 'transforming', 'various', 'industries', 'making', 'easier', 'efficient', 'healthcare', 'finance', 'aipowered', 'systems', 'becoming', 'indispensable', 'decisionmaking', 'technologies', 'learning', 'machine', 'learning', 'better', 'predictions', 'automated', 'processes', 'enhanced', 'analysis']\n",
      "\n",
      "Numbers found in text: []\n",
      "\n",
      "Capitalized words: []\n",
      "\n",
      "Alphabetic words: ['artificial', 'intelligence', 'is', 'transforming', 'various', 'industries', 'making', 'tasks', 'easier', 'and', 'more', 'efficient', 'from', 'healthcare', 'to', 'finance', 'aipowered', 'systems', 'are', 'becoming', 'indispensable', 'tools', 'for', 'decisionmaking', 'technologies', 'like', 'deep', 'learning', 'and', 'machine', 'learning', 'allow', 'for', 'better', 'predictions', 'automated', 'processes', 'and', 'enhanced', 'data', 'analysis']\n",
      "\n",
      "Words starting with a vowel: ['artificial', 'intelligence', 'is', 'industries', 'easier', 'and', 'efficient', 'aipowered', 'are', 'indispensable', 'and', 'allow', 'automated', 'and', 'enhanced', 'analysis']\n"
     ]
    }
   ],
   "source": [
    "# 1. Use regular expressions to extract:\n",
    "# a. All words with more than 5 letters\n",
    "long_words = re.findall(r'\\b\\w{6,}\\b', text_clean)\n",
    "print(\"\\nWords with more than 5 letters:\", long_words)\n",
    "\n",
    "# b. All numbers \n",
    "numbers = re.findall(r'\\b\\d+\\b', text_clean)\n",
    "print(\"\\nNumbers found in text:\", numbers)\n",
    "\n",
    "# c. All capitalized words\n",
    "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', text_clean)\n",
    "print(\"\\nCapitalized words:\", capitalized_words)\n",
    "\n",
    "# 2. Use text splitting techniques:\n",
    "# a. Split the text into words containing only alphabets \n",
    "alphabetic_words = re.findall(r'\\b[a-zA-Z]+\\b', text_clean)\n",
    "print(\"\\nAlphabetic words:\", alphabetic_words)\n",
    "\n",
    "# b. Extract words starting with a vowel\n",
    "vowel_words = re.findall(r'\\b[aeiouAEIOU]\\w*\\b', text_clean)\n",
    "print(\"\\nWords starting with a vowel:\", vowel_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Custom TokenizaƟon & Regex-based Text Cleaning\n",
    "1. Take original text from QuesƟon 1.\n",
    "2. Write a custom tokenizaƟon funcƟon that:\n",
    "a. Removes punctuaƟon and special symbols, but keeps contracƟons (e.g.,\n",
    "\"isn't\" should not be split into \"is\" and \"n't\").\n",
    "b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains\n",
    "a single token).\n",
    "c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\"\n",
    "should remain as is).\n",
    "3. Use Regex SubsƟtuƟons (re.sub) to:\n",
    "a. Replace email addresses with '<EMAIL>' placeholder.\n",
    "b. Replace URLs with '<URL>' placeholder.\n",
    "c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with\n",
    "'<PHONE>' placeholder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom Tokenized Text: ['artificial', 'intelligence', 'is', 'transforming', 'various', 'industries', 'making', 'tasks', 'easier', 'and', 'more', 'efficient', 'from', 'healthcare', 'to', 'finance', 'ai-powered', 'systems', 'are', 'becoming', 'indispensable', 'tools', 'for', 'decision-making', 'technologies', 'like', 'deep', 'learning', 'and', 'machine', 'learning', 'allow', 'for', 'better', 'predictions', 'automated', 'processes', 'and', 'enhanced', 'data', 'analysis']\n",
      "\n",
      "Text with Emails, URLs, and Phone Numbers Replaced:\n",
      "\n",
      "Artificial Intelligence is transforming various industries, making tasks easier and more efficient. \n",
      "From healthcare to finance, AI-powered systems are becoming indispensable tools for decision-making. \n",
      "Technologies like deep learning and machine learning allow for better predictions, automated processes, and enhanced data analysis.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Custom tokenization function\n",
    "def custom_tokenizer(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r'[^\\w\\s\\'-]', '', text)\n",
    "    return word_tokenize(text)\n",
    "\n",
    "custom_tokens = custom_tokenizer(text)\n",
    "print(\"\\nCustom Tokenized Text:\", custom_tokens)\n",
    "\n",
    "# 2. Regex substitutions for cleaning\n",
    "# a. Replace email addresses with '<EMAIL>'\n",
    "text_with_emails = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', '<EMAIL>', text)\n",
    "# b. Replace URLs with '<URL>'\n",
    "text_with_urls = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '<URL>', text_with_emails)\n",
    "# c. Replace phone numbers with '<PHONE>'\n",
    "text_cleaned = re.sub(r'(\\+?\\d{1,2}\\s?)?(\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4})', '<PHONE>', text_with_urls)\n",
    "\n",
    "print(\"\\nText with Emails, URLs, and Phone Numbers Replaced:\")\n",
    "print(text_cleaned)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
